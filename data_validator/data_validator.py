# data_validator.py
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class DataValidator:
    def __init__(self, data_file):
        self.data = pd.read_csv(data_file)
        self.predictions_history = {}
        self.performance_metrics = {}
        self.validation_results = {}
        
    def validate_and_clean_data(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥ØµÙ„Ø§Ø­Ù‡Ø§"""
        print("ğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        required_columns = ['HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'Date']
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            print(f"âŒ Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_columns}")
            return False
        
        # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ±Ù‚
        self.data['HomeTeam'] = self.data['HomeTeam'].astype(str).str.strip().str.title()
        self.data['AwayTeam'] = self.data['AwayTeam'].astype(str).str.strip().str.title()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
        initial_count = len(self.data)
        self.data = self.data.dropna(subset=['HomeTeam', 'AwayTeam', 'FTHG', 'FTAG'])
        self.data = self.data[self.data['HomeTeam'] != self.data['AwayTeam']]
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        self._clean_numeric_data()
        
        print(f"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {initial_count} â†’ {len(self.data)} Ù…Ø¨Ø§Ø±Ø§Ø©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self._analyze_data_quality()
        
        return True
    
    def _clean_numeric_data(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©"""
        numeric_columns = ['FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR']
        
        for col in numeric_columns:
            if col in self.data.columns:
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø© Ø¨ØµÙØ±
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce').fillna(0).astype(int)
                # Ø¶Ù…Ø§Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø³Ø§Ù„Ø¨Ø©
                self.data[col] = self.data[col].clip(lower=0)
    
    def _analyze_data_quality(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        print(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª: {len(self.data)}")
        print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ±Ù‚: {len(set(self.data['HomeTeam']) | set(self.data['AwayTeam']))}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        if 'Date' in self.data.columns:
            try:
                self.data['Date'] = pd.to_datetime(self.data['Date'], errors='coerce')
                valid_dates = self.data['Date'].notna()
                date_range = self.data[valid_dates]['Date']
                if len(date_range) > 0:
                    print(f"â€¢ Ù†Ø·Ø§Ù‚ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®: {date_range.min().strftime('%Y-%m-%d')} Ø¥Ù„Ù‰ {date_range.max().strftime('%Y-%m-%d')}")
            except:
                print("â€¢ Ù†Ø·Ø§Ù‚ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®: ØºÙŠØ± Ù…ØªÙˆÙØ±")
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        result_dist = self.data['FTR'].value_counts()
        print(f"â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: H={result_dist.get('H', 0)}, D={result_dist.get('D', 0)}, A={result_dist.get('A', 0)}")
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
        avg_goals = (self.data['FTHG'].mean() + self.data['FTAG'].mean())
        print(f"â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {avg_goals:.2f} Ù„ÙƒÙ„ Ù…Ø¨Ø§Ø±Ø§Ø©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„
        self._analyze_data_completeness()
    
    def _analyze_data_completeness(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        print(f"\nğŸ“‹ ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        total_matches = len(self.data)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        additional_stats = ['HS', 'AS', 'HST', 'AST']
        available_stats = [stat for stat in additional_stats if stat in self.data.columns]
        
        print(f"â€¢ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(available_stats)} Ù…Ù† {len(additional_stats)}")
        
        for stat in available_stats:
            completeness = (self.data[stat].notna().sum() / total_matches) * 100
            print(f"  - {stat}: {completeness:.1f}% Ù…ÙƒØªÙ…Ù„")
    
    def record_prediction(self, prediction_id, prediction_data, match_info):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹"""
        self.predictions_history[prediction_id] = {
            'prediction': prediction_data,
            'match_info': match_info,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'evaluated': False
        }
        print(f"âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤: {prediction_id}")
    
    def validate_predictions_accuracy(self, actual_results_file=None, use_internal_data=True):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
        print("\nğŸ¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª...")
        
        if not self.predictions_history:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø³Ø¬Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚")
            return None
        
        evaluation_results = {}
        total_predictions = len(self.predictions_history)
        evaluated_count = 0
        
        for pred_id, pred_data in self.predictions_history.items():
            match_info = pred_data['match_info']
            home_team = match_info['home_team']
            away_team = match_info['away_team']
            match_date = match_info.get('date')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©
            actual_result = self._find_actual_result(home_team, away_team, match_date, actual_results_file, use_internal_data)
            
            if actual_result:
                accuracy_metrics = self._calculate_prediction_accuracy(pred_data['prediction'], actual_result)
                evaluation_results[pred_id] = {
                    'prediction': pred_data['prediction'],
                    'actual_result': actual_result,
                    'accuracy_metrics': accuracy_metrics,
                    'match_info': match_info
                }
                self.predictions_history[pred_id]['evaluated'] = True
                evaluated_count += 1
        
        print(f"âœ… ØªÙ… ØªÙ‚ÙŠÙŠÙ… {evaluated_count} Ù…Ù† Ø£ØµÙ„ {total_predictions} ØªÙ†Ø¨Ø¤")
        
        if evaluated_count > 0:
            overall_metrics = self._calculate_overall_accuracy(evaluation_results)
            self.performance_metrics = overall_metrics
            self.validation_results = evaluation_results
            
            self._generate_accuracy_report(overall_metrics, evaluation_results)
            return overall_metrics
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙØ¹Ù„ÙŠØ© Ù„Ø£ÙŠ ØªÙ†Ø¨Ø¤")
            return None
    
    def _find_actual_result(self, home_team, away_team, match_date, actual_results_file, use_internal_data):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ù…Ø¨Ø§Ø±Ø§Ø©"""
        if use_internal_data and not actual_results_file:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
            match_condition = (
                (self.data['HomeTeam'].str.contains(home_team, case=False, na=False)) |
                (self.data['HomeTeam'] == home_team)
            ) & (
                (self.data['AwayTeam'].str.contains(away_team, case=False, na=False)) |
                (self.data['AwayTeam'] == away_team)
            )
            
            if match_date:
                try:
                    match_date = pd.to_datetime(match_date)
                    match_condition &= (self.data['Date'] == match_date)
                except:
                    pass
            
            matches = self.data[match_condition]
            
            if len(matches) > 0:
                match = matches.iloc[0]
                return {
                    'home_goals': match['FTHG'],
                    'away_goals': match['FTAG'],
                    'result': match['FTR'],
                    'home_team': match['HomeTeam'],
                    'away_team': match['AwayTeam'],
                    'date': match['Date'] if 'Date' in match else None
                }
        
        elif actual_results_file:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ
            try:
                actual_data = pd.read_csv(actual_results_file)
                # ØªØ·Ø¨ÙŠÙ‚ Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¨Ø­Ø« (ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµÙ‡ Ø­Ø³Ø¨ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù)
                match_condition = (
                    (actual_data['HomeTeam'] == home_team) &
                    (actual_data['AwayTeam'] == away_team)
                )
                matches = actual_data[match_condition]
                
                if len(matches) > 0:
                    match = matches.iloc[0]
                    return {
                        'home_goals': match['FTHG'],
                        'away_goals': match['FTAG'],
                        'result': match['FTR'],
                        'home_team': match['HomeTeam'],
                        'away_team': match['AwayTeam'],
                        'date': match['Date'] if 'Date' in match else None
                    }
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {e}")
        
        return None
    
    def _calculate_prediction_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙØ±Ø¯ÙŠ"""
        metrics = {}
        
        # Ø¯Ù‚Ø© Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© (H/D/A)
        pred_result = self._determine_predicted_result(prediction)
        actual_FTR = actual_result['result']
        
        metrics['result_accuracy'] = 1 if pred_result == actual_FTR else 0
        metrics['predicted_result'] = pred_result
        metrics['actual_result'] = actual_FTR
        
        # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„ÙÙˆØ²/Ø§Ù„ØªØ¹Ø§Ø¯Ù„/Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        metrics['outcome_accuracy'] = self._calculate_outcome_accuracy(prediction, actual_result)
        
        # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
        metrics['score_accuracy'] = self._calculate_score_accuracy(prediction, actual_result)
        
        # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„ÙØ§Ø±Ù‚
        metrics['goal_difference_accuracy'] = self._calculate_goal_difference_accuracy(prediction, actual_result)
        
        # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø£ÙƒØ«Ø± Ù…Ù† 2.5 Ù‡Ø¯Ù
        metrics['over_under_accuracy'] = self._calculate_over_under_accuracy(prediction, actual_result)
        
        # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙØ±ÙŠÙ‚ÙŠÙ†
        metrics['both_teams_score_accuracy'] = self._calculate_both_teams_score_accuracy(prediction, actual_result)
        
        return metrics
    
    def _determine_predicted_result(self, prediction):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤"""
        if 'multiple_predictions' in prediction:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ ØªÙ†Ø¨Ø¤
            best_pred = prediction['multiple_predictions'][0]
            home_goals = best_pred['home_goals']
            away_goals = best_pred['away_goals']
        elif 'score_prediction' in prediction:
            home_goals = prediction['score_prediction']['home_goals']
            away_goals = prediction['score_prediction']['away_goals']
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
            probs = prediction.get('probabilities', {})
            if probs:
                max_prob = max(probs.get('home_win', 0), probs.get('draw', 0), probs.get('away_win', 0))
                if max_prob == probs.get('home_win', 0):
                    return 'H'
                elif max_prob == probs.get('away_win', 0):
                    return 'A'
                else:
                    return 'D'
            return 'D'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
        
        if home_goals > away_goals:
            return 'H'
        elif away_goals > home_goals:
            return 'A'
        else:
            return 'D'
    
    def _calculate_outcome_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© (ÙÙˆØ²/ØªØ¹Ø§Ø¯Ù„/Ø®Ø³Ø§Ø±Ø©)"""
        pred_result = self._determine_predicted_result(prediction)
        actual_FTR = actual_result['result']
        return 1 if pred_result == actual_FTR else 0
    
    def _calculate_score_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©"""
        if 'multiple_predictions' in prediction:
            best_pred = prediction['multiple_predictions'][0]
            pred_home = best_pred['home_goals']
            pred_away = best_pred['away_goals']
        elif 'score_prediction' in prediction:
            pred_home = prediction['score_prediction']['home_goals']
            pred_away = prediction['score_prediction']['away_goals']
        else:
            return 0  # Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆÙ‚Ø¹Ø§Øª Ù„Ù„Ø£Ù‡Ø¯Ø§Ù
        
        actual_home = actual_result['home_goals']
        actual_away = actual_result['away_goals']
        
        return 1 if (pred_home == actual_home and pred_away == actual_away) else 0
    
    def _calculate_goal_difference_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ÙØ§Ø±Ù‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù"""
        if 'multiple_predictions' in prediction:
            best_pred = prediction['multiple_predictions'][0]
            pred_diff = best_pred['home_goals'] - best_pred['away_goals']
        elif 'score_prediction' in prediction:
            pred_diff = prediction['score_prediction']['home_goals'] - prediction['score_prediction']['away_goals']
        else:
            return 0
        
        actual_diff = actual_result['home_goals'] - actual_result['away_goals']
        
        # ØªØ¹ØªØ¨Ø± ØµØ­ÙŠØ­Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙØ§Ø±Ù‚ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if (pred_diff > 0 and actual_diff > 0) or (pred_diff < 0 and actual_diff < 0) or (pred_diff == 0 and actual_diff == 0):
            return 1
        return 0
    
    def _calculate_over_under_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø£ÙƒØ«Ø±/Ø£Ù‚Ù„ Ù…Ù† 2.5 Ù‡Ø¯Ù"""
        total_goals = actual_result['home_goals'] + actual_result['away_goals']
        actual_over = total_goals > 2.5
        
        pred_probs = prediction.get('probabilities', {})
        pred_over_prob = pred_probs.get('over_2_5', 0.5)
        pred_over = pred_over_prob > 0.5
        
        return 1 if pred_over == actual_over else 0
    
    def _calculate_both_teams_score_accuracy(self, prediction, actual_result):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙØ±ÙŠÙ‚ÙŠÙ†"""
        actual_both_score = actual_result['home_goals'] > 0 and actual_result['away_goals'] > 0
        
        pred_probs = prediction.get('probabilities', {})
        pred_both_prob = pred_probs.get('both_teams_score', 0.5)
        pred_both_score = pred_both_prob > 0.5
        
        return 1 if pred_both_score == actual_both_score else 0
    
    def _calculate_overall_accuracy(self, evaluation_results):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        total_predictions = len(evaluation_results)
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        result_accuracy = []
        outcome_accuracy = []
        score_accuracy = []
        goal_diff_accuracy = []
        over_under_accuracy = []
        both_teams_accuracy = []
        
        for pred_id, data in evaluation_results.items():
            metrics = data['accuracy_metrics']
            result_accuracy.append(metrics['result_accuracy'])
            outcome_accuracy.append(metrics['outcome_accuracy'])
            score_accuracy.append(metrics['score_accuracy'])
            goal_diff_accuracy.append(metrics['goal_difference_accuracy'])
            over_under_accuracy.append(metrics['over_under_accuracy'])
            both_teams_accuracy.append(metrics['both_teams_score_accuracy'])
        
        overall_metrics = {
            'total_predictions': total_predictions,
            'result_accuracy': np.mean(result_accuracy),
            'outcome_accuracy': np.mean(outcome_accuracy),
            'score_accuracy': np.mean(score_accuracy),
            'goal_difference_accuracy': np.mean(goal_diff_accuracy),
            'over_under_accuracy': np.mean(over_under_accuracy),
            'both_teams_score_accuracy': np.mean(both_teams_accuracy),
            'overall_accuracy': np.mean([
                np.mean(result_accuracy),
                np.mean(outcome_accuracy),
                np.mean(goal_diff_accuracy)
            ])
        }
        
        return overall_metrics
    
    def _generate_accuracy_report(self, overall_metrics, evaluation_results):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¹Ù† Ø§Ù„Ø¯Ù‚Ø©"""
        print("\n" + "="*80)
        print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„")
        print("="*80)
        
        print(f"\nğŸ“ˆ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©: {overall_metrics['overall_accuracy']:.1%}")
        print(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙŠÙ…Ø©: {overall_metrics['total_predictions']}")
        
        print(f"\nğŸ¯ ØªÙØµÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø©:")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© (H/D/A): {overall_metrics['result_accuracy']:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„ÙÙˆØ²/Ø§Ù„ØªØ¹Ø§Ø¯Ù„/Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {overall_metrics['outcome_accuracy']:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©: {overall_metrics['score_accuracy']:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ÙØ§Ø±Ù‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {overall_metrics['goal_difference_accuracy']:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø£ÙƒØ«Ø±/Ø£Ù‚Ù„ Ù…Ù† 2.5 Ù‡Ø¯Ù: {overall_metrics['over_under_accuracy']:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙØ±ÙŠÙ‚ÙŠÙ†: {overall_metrics['both_teams_score_accuracy']:.1%}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        self._analyze_by_result_type(evaluation_results)
        
        # Ø£ÙØ¶Ù„ ÙˆØ£Ø³ÙˆØ£ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        self._show_extreme_predictions(evaluation_results)
    
    def _analyze_by_result_type(self, evaluation_results):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©"""
        result_types = {'H': [], 'D': [], 'A': []}
        
        for pred_id, data in evaluation_results.items():
            actual_result = data['actual_result']['result']
            accuracy = data['accuracy_metrics']['result_accuracy']
            result_types[actual_result].append(accuracy)
        
        print(f"\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©:")
        for result_type, accuracies in result_types.items():
            if accuracies:
                avg_accuracy = np.mean(accuracies)
                count = len(accuracies)
                result_name = {'H': 'ÙÙˆØ² Ø§Ù„Ù…Ù†Ø²Ù„', 'D': 'ØªØ¹Ø§Ø¯Ù„', 'A': 'ÙÙˆØ² Ø§Ù„Ø¶ÙŠÙ'}[result_type]
                print(f"â€¢ {result_name}: {avg_accuracy:.1%} ({count} Ù…Ø¨Ø§Ø±Ø§Ø©)")
    
    def _show_extreme_predictions(self, evaluation_results):
        """Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ ÙˆØ£Ø³ÙˆØ£ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        predictions_with_accuracy = []
        
        for pred_id, data in evaluation_results.items():
            accuracy = data['accuracy_metrics']['result_accuracy']
            match_info = data['match_info']
            prediction = data['prediction']
            actual = data['actual_result']
            
            predictions_with_accuracy.append({
                'pred_id': pred_id,
                'accuracy': accuracy,
                'home_team': match_info['home_team'],
                'away_team': match_info['away_team'],
                'predicted_result': data['accuracy_metrics']['predicted_result'],
                'actual_result': data['accuracy_metrics']['actual_result'],
                'predicted_score': self._get_predicted_score(prediction),
                'actual_score': f"{actual['home_goals']}-{actual['away_goals']}"
            })
        
        # Ø£ÙØ¶Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        best_predictions = sorted(predictions_with_accuracy, key=lambda x: x['accuracy'], reverse=True)[:3]
        worst_predictions = sorted(predictions_with_accuracy, key=lambda x: x['accuracy'])[:3]
        
        print(f"\nğŸ† Ø£ÙØ¶Ù„ 3 ØªÙ†Ø¨Ø¤Ø§Øª:")
        for i, pred in enumerate(best_predictions, 1):
            print(f"  {i}. {pred['home_team']} vs {pred['away_team']}: "
                  f"Ù…ØªÙˆÙ‚Ø¹ {pred['predicted_result']} ({pred['predicted_score']}) | "
                  f"ÙØ¹Ù„ÙŠ {pred['actual_result']} ({pred['actual_score']}) | "
                  f"Ø¯Ù‚Ø©: {pred['accuracy']:.0%}")
        
        print(f"\nğŸ“‰ Ø£Ø³ÙˆØ£ 3 ØªÙ†Ø¨Ø¤Ø§Øª:")
        for i, pred in enumerate(worst_predictions, 1):
            print(f"  {i}. {pred['home_team']} vs {pred['away_team']}: "
                  f"Ù…ØªÙˆÙ‚Ø¹ {pred['predicted_result']} ({pred['predicted_score']}) | "
                  f"ÙØ¹Ù„ÙŠ {pred['actual_result']} ({pred['actual_score']}) | "
                  f"Ø¯Ù‚Ø©: {pred['accuracy']:.0%}")
    
    def _get_predicted_score(self, prediction):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙƒØ³Ù„Ø³Ù„Ø©"""
        if 'multiple_predictions' in prediction:
            best_pred = prediction['multiple_predictions'][0]
            return f"{best_pred['home_goals']}-{best_pred['away_goals']}"
        elif 'score_prediction' in prediction:
            return f"{prediction['score_prediction']['home_goals']}-{prediction['score_prediction']['away_goals']}"
        else:
            return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
    
    def evaluate_model_performance(self, time_period=None):
        """ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø²Ù…Ù†"""
        print("\nğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
        
        if not self.validation_results:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù‚Ù‚ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡")
            return
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ø²Ù…Ù† (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ù…ØªØ§Ø­Ø©)
        dated_predictions = []
        for pred_id, data in self.validation_results.items():
            match_info = data['match_info']
            if 'date' in match_info and match_info['date']:
                try:
                    date = pd.to_datetime(match_info['date'])
                    accuracy = data['accuracy_metrics']['result_accuracy']
                    dated_predictions.append({'date': date, 'accuracy': accuracy})
                except:
                    continue
        
        if dated_predictions:
            dated_df = pd.DataFrame(dated_predictions)
            dated_df = dated_df.sort_values('date')
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…ØªØ­Ø±Ùƒ Ù„Ù„Ø¯Ù‚Ø©
            dated_df['moving_avg'] = dated_df['accuracy'].rolling(window=5, min_periods=1).mean()
            
            print(f"\nğŸ“… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø²Ù…Ù†ÙŠ:")
            print(f"â€¢ Ø£ÙˆÙ„ ØªÙ†Ø¨Ø¤: {dated_df['date'].min().strftime('%Y-%m-%d')}")
            print(f"â€¢ Ø¢Ø®Ø± ØªÙ†Ø¨Ø¤: {dated_df['date'].max().strftime('%Y-%m-%d')}")
            print(f"â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø²Ù…Ù†ÙŠ: {dated_df['accuracy'].mean():.1%}")
            print(f"â€¢ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø¡: {'ØªØ­Ø³Ù†' if dated_df['moving_avg'].iloc[-1] > dated_df['moving_avg'].iloc[0] else 'ØªØ±Ø§Ø¬Ø¹'}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„ÙØ±Ù‚
        self._analyze_performance_by_team_strength()
    
    def _analyze_performance_by_team_strength(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„ÙØ±Ù‚"""
        print(f"\nğŸ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ù‚ÙˆØ© Ø§Ù„ÙØ±Ù‚:")
        
        # Ù‡Ø°Ø§ ÙŠØªØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù† Ù‚ÙˆØ© Ø§Ù„ÙØ±Ù‚ - ÙŠÙ…ÙƒÙ† ØªÙˆØ³ÙŠØ¹Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
        strength_categories = {
            'Ù…Ø¨Ø§Ø±ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø©': ['Man City', 'Liverpool', 'Arsenal', 'Chelsea', 'Man United', 'Tottenham'],
            'ÙØ±Ù‚ Ù…ØªÙˆØ³Ø·Ø©': ['Newcastle', 'West Ham', 'Brighton', 'Aston Villa', 'Crystal Palace'],
            'ÙØ±Ù‚ ØµØºÙŠØ±Ø©': ['Bournemouth', 'Brentford', 'Fulham', 'Wolves', 'Everton']
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· - ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        print("â€¢ Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ­Ù„ÙŠÙ„ Ù‚ÙˆØ© Ø§Ù„ÙØ±Ù‚ ÙŠØªØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù† ØªØµÙ†ÙŠÙ Ø§Ù„ÙØ±Ù‚")
    
    def generate_performance_report(self, save_to_file=False):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„"""
        if not self.performance_metrics:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ù…ØªØ§Ø­Ø©")
            return
        
        report = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'performance_metrics': self.performance_metrics,
            'total_evaluated_predictions': len(self.validation_results),
            'summary': self._generate_performance_summary()
        }
        
        print("\n" + "="*80)
        print("ğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„")
        print("="*80)
        print(f"\n{report['summary']}")
        
        if save_to_file:
            filename = f"model_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {filename}")
        
        return report
    
    def _generate_performance_summary(self):
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        metrics = self.performance_metrics
        
        summary = f"""
ğŸ¯ Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤:

â€¢ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©: {metrics['overall_accuracy']:.1%}
â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙŠÙ…Ø©: {metrics['total_predictions']}

ğŸ“Š ØªÙØµÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø©:
  â€¢ ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {metrics['result_accuracy']:.1%}
  â€¢ ØªÙˆÙ‚Ø¹ Ø§Ù„ÙÙˆØ²/Ø§Ù„ØªØ¹Ø§Ø¯Ù„/Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {metrics['outcome_accuracy']:.1%}
  â€¢ ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©: {metrics['score_accuracy']:.1%}
  â€¢ ØªÙˆÙ‚Ø¹ ÙØ§Ø±Ù‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {metrics['goal_difference_accuracy']:.1%}
  â€¢ ØªÙˆÙ‚Ø¹ Ø£ÙƒØ«Ø±/Ø£Ù‚Ù„ Ù…Ù† 2.5 Ù‡Ø¯Ù: {metrics['over_under_accuracy']:.1%}
  â€¢ ØªÙˆÙ‚Ø¹ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙØ±ÙŠÙ‚ÙŠÙ†: {metrics['both_teams_score_accuracy']:.1%}

ğŸ“ˆ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {'Ù…Ù…ØªØ§Ø²' if metrics['overall_accuracy'] > 0.6 else 'Ø¬ÙŠØ¯' if metrics['overall_accuracy'] > 0.5 else 'Ø¨Ø­Ø§Ø¬Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†'}
"""
        return summary
    
    def compare_with_baseline(self, baseline_method='random'):
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø·Ø±Ù‚ baseline"""
        print(f"\nğŸ” Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ {baseline_method} baseline...")
        
        if baseline_method == 'random':
            # Ø¯Ù‚Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù†Ø¸Ø±ÙŠØ©
            baseline_accuracy = 0.33  # 33% Ù„Ø«Ù„Ø§Ø« Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ØªÙ…Ù„Ø©
        elif baseline_method == 'home_win_bias':
            # Ø§ÙØªØ±Ø§Ø¶ ÙÙˆØ² Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø²Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹
            home_win_rate = len(self.data[self.data['FTR'] == 'H']) / len(self.data)
            baseline_accuracy = home_win_rate
        else:
            baseline_accuracy = 0.33
        
        model_accuracy = self.performance_metrics.get('result_accuracy', 0)
        improvement = ((model_accuracy - baseline_accuracy) / baseline_accuracy) * 100
        
        print(f"â€¢ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_accuracy:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© {baseline_method} baseline: {baseline_accuracy:.1%}")
        print(f"â€¢ Ø§Ù„ØªØ­Ø³Ù†: {improvement:+.1f}%")
        
        if improvement > 0:
            print("âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙÙˆÙ‚ Ø¹Ù„Ù‰ baseline")
        else:
            print("âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† Ù„ØªØ¬Ø§ÙˆØ² baseline")

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ù…Ø­Ø³Ù†
    validator = DataValidator("data/football-data/combined_seasons_data.csv")
    
    if validator.validate_and_clean_data():
        print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        sample_predictions = [
            {
                'id': 'pred_001',
                'prediction': {
                    'multiple_predictions': [
                        {'home_goals': 2, 'away_goals': 1, 'probability': 0.15, 'type': 'Ø§Ù„Ø£ÙƒØ«Ø± ØªØ±Ø¬ÙŠØ­Ø§Ù‹'}
                    ],
                    'probabilities': {
                        'home_win': 0.55, 'draw': 0.25, 'away_win': 0.20,
                        'over_2_5': 0.65, 'both_teams_score': 0.70
                    }
                },
                'match_info': {
                    'home_team': 'Man City',
                    'away_team': 'Liverpool', 
                    'date': '2023-05-15'
                }
            }
        ]
        
        for pred in sample_predictions:
            validator.record_prediction(pred['id'], pred['prediction'], pred['match_info'])
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        accuracy_results = validator.validate_predictions_accuracy()
        
        if accuracy_results:
            # ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            validator.evaluate_model_performance()
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
            validator.generate_performance_report(save_to_file=True)
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ baseline
            validator.compare_with_baseline('random')