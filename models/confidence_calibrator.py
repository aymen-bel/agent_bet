# models/confidence_calibrator.py - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙˆØ§Ù„Ù…ÙƒØªÙ…Ù„Ø©

import numpy as np
from typing import Dict, List, Tuple
import logging
from sklearn.isotonic import IsotonicRegression
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
import json

class ConfidenceCalibrator:
    def __init__(self):
        self.calibration_data = []
        self.isotonic_model = None
        self.is_calibrated = False
        self.calibration_report = {}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def collect_calibration_data(self, predictions: List[Dict], actual_results: List[Dict]):
        """Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
        try:
            if len(predictions) != len(actual_results):
                self.logger.warning(f"âš ï¸  Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ({len(predictions)}) Ù„Ø§ ÙŠØ³Ø§ÙˆÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(actual_results)})")
                min_len = min(len(predictions), len(actual_results))
                predictions = predictions[:min_len]
                actual_results = actual_results[:min_len]
            
            for pred, actual in zip(predictions, actual_results):
                if pred and actual:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙØ§Ø±ØºØ©
                    self.calibration_data.append((pred, actual))
            
            self.logger.info(f"ğŸ“Š ØªÙ… Ø¬Ù…Ø¹ {len(self.calibration_data)} Ø¹ÙŠÙ†Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠØ±Ø©")
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")
    
    def fit(self):
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        try:
            if len(self.calibration_data) < 10:
                self.logger.warning("âš ï¸  Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© ØºÙŠØ± ÙƒØ§ÙÙŠØ©")
                return False
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ©
            confidences = []
            actuals = []
            
            for pred, actual in self.calibration_data:
                confidence = pred.get('confidence', 0.5)
                is_correct = self._is_prediction_correct(pred, actual)
                
                confidences.append(confidence)
                actuals.append(1 if is_correct else 0)
            
            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Isotonic Regression Ù„Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            self.isotonic_model = IsotonicRegression(out_of_bounds='clip')
            self.isotonic_model.fit(confidences, actuals)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            self._generate_calibration_report(confidences, actuals)
            
            self.is_calibrated = True
            self.logger.info("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø©")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")
            return False
    
    def _is_prediction_correct(self, prediction: Dict, actual_result: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            if not prediction or not actual_result:
                return False
            
            pred_home = prediction.get('home_goals', 0)
            pred_away = prediction.get('away_goals', 0)
            actual_home = actual_result.get('home_goals', 0)
            actual_away = actual_result.get('away_goals', 0)
            
            # Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ ØµØ­ÙŠØ­ Ø¥Ø°Ø§ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙØ§Ø¦Ø² Ø£Ùˆ Ø§Ù„ØªØ¹Ø§Ø¯Ù„
            if (pred_home > pred_away and actual_home > actual_away) or \
               (pred_home < pred_away and actual_home < actual_away) or \
               (pred_home == pred_away and actual_home == actual_away):
                return True
            
            # Ø£ÙŠØ¶Ø§Ù‹ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ ØµØ­ÙŠØ­ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙØ±Ù‚ ÙÙŠ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ØµØºÙŠØ±
            goal_diff_pred = abs(pred_home - pred_away)
            goal_diff_actual = abs(actual_home - actual_away)
            
            if abs(goal_diff_pred - goal_diff_actual) <= 1:
                return True
            
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return False
    
    def calibrate_confidence(self, confidence: float, prediction_type: str = "general") -> float:
        """Ù…Ø¹Ø§ÙŠØ±Ø© Ù‚ÙŠÙ…Ø© Ø§Ù„Ø«Ù‚Ø©"""
        try:
            if not self.is_calibrated or self.isotonic_model is None:
                return confidence
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            calibrated = self.isotonic_model.predict([confidence])[0]
            return max(0.0, min(1.0, calibrated))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {e}")
            return confidence
    
    def _generate_calibration_report(self, confidences: List[float], actuals: List[int]):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            prob_true, prob_pred = calibration_curve(actuals, confidences, n_bins=10, strategy='uniform')
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
            calibration_error = np.mean(np.abs(prob_true - prob_pred))
            brier_score = np.mean([(actual - pred) ** 2 for actual, pred in zip(actuals, confidences)])
            
            # Ø­Ø³Ø§Ø¨ Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„Ø«Ù‚Ø©
            confidence_reliability = {}
            for conf_bin in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:
                mask = [c >= conf_bin and c < conf_bin + 0.2 for c in confidences]
                if sum(mask) > 0:
                    bin_accuracy = np.mean([actuals[i] for i, m in enumerate(mask) if m])
                    confidence_reliability[f"{conf_bin:.1f}-{conf_bin+0.2:.1f}"] = {
                        'count': sum(mask),
                        'accuracy': bin_accuracy,
                        'avg_confidence': np.mean([confidences[i] for i, m in enumerate(mask) if m])
                    }
            
            self.calibration_report = {
                'calibration_error': float(calibration_error),
                'brier_score': float(brier_score),
                'reliability': confidence_reliability,
                'total_samples': len(confidences),
                'overall_accuracy': np.mean(actuals)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")
    
    def _classify_prediction_type(self, prediction: Dict) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        try:
            home_goals = prediction.get('home_goals', 0)
            away_goals = prediction.get('away_goals', 0)
            
            if home_goals == away_goals:
                return "draw"
            elif abs(home_goals - away_goals) <= 1:
                return "close_win"
            elif abs(home_goals - away_goals) <= 3:
                return "comfortable_win"
            else:
                return "big_win"
                
        except Exception as e:
            self.logger.warning(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return "unknown"
    
    def generate_calibration_report(self) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        if not self.calibration_report:
            return "ğŸ“Š Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© Ù…ØªØ§Ø­Ø©"
        
        report = [
            "ğŸ“ˆ ØªÙ‚Ø±ÙŠØ± Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø©",
            "=" * 40,
            f"â€¢ Ø®Ø·Ø£ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {self.calibration_report.get('calibration_error', 0):.3f}",
            f"â€¢ Ø¯Ø±Ø¬Ø© Ø¨Ø±Ø§ÙŠÙŠØ±: {self.calibration_report.get('brier_score', 0):.3f}",
            f"â€¢ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {self.calibration_report.get('overall_accuracy', 0):.1%}",
            f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {self.calibration_report.get('total_samples', 0)}",
            "",
            "ğŸ“Š Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„Ø«Ù‚Ø©:"
        ]
        
        reliability = self.calibration_report.get('reliability', {})
        for bin_range, stats in reliability.items():
            report.append(f"â€¢ {bin_range}: {stats['accuracy']:.1%} Ø¯Ù‚Ø© ({stats['count']} Ø¹ÙŠÙ†Ø©)")
        
        return "\n".join(report)
    
    def save_calibration_models(self, filepath: str):
        """Ø­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        try:
            calibration_data = {
                'is_calibrated': self.is_calibrated,
                'calibration_report': self.calibration_report,
                'calibration_data_sample': self.calibration_data[:10] if self.calibration_data else []
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(calibration_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© ÙÙŠ: {filepath}")
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")
    
    def load_calibration_models(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                calibration_data = json.load(f)
            
            self.is_calibrated = calibration_data.get('is_calibrated', False)
            self.calibration_report = calibration_data.get('calibration_report', {})
            
            self.logger.info(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ù…Ù†: {filepath}")
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")