# prediction_tracker.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import os

class PredictionTracker:
    def __init__(self, tracking_file="prediction_tracking.json"):
        self.tracking_file = tracking_file
        self.predictions = self.load_predictions()
        
    def load_predictions(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù…Ø³Ø¬Ù„Ø©"""
        if os.path.exists(self.tracking_file):
            with open(self.tracking_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_predictions(self):
        """Ø­ÙØ¸ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª"""
        with open(self.tracking_file, 'w', encoding='utf-8') as f:
            json.dump(self.predictions, f, ensure_ascii=False, indent=2)
    
    def record_prediction(self, match_id, prediction_data, actual_result=None):
        """ØªØ³Ø¬ÙŠÙ„ ØªÙˆÙ‚Ø¹ Ø¬Ø¯ÙŠØ¯"""
        prediction_record = {
            'timestamp': datetime.now().isoformat(),
            'prediction': prediction_data,
            'actual_result': actual_result,
            'evaluated': actual_result is not None
        }
        
        self.predictions[match_id] = prediction_record
        self.save_predictions()
        
        print(f"âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ ØªÙˆÙ‚Ø¹ Ù„Ù„Ù…Ø¨Ø§Ø±Ø§Ø© {match_id}")
    
    def update_actual_result(self, match_id, actual_result):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ù…Ø¨Ø§Ø±Ø§Ø©"""
        if match_id in self.predictions:
            self.predictions[match_id]['actual_result'] = actual_result
            self.predictions[match_id]['evaluated'] = True
            self.save_predictions()
            print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ù…Ø¨Ø§Ø±Ø§Ø© {match_id}")
        else:
            print(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªÙˆÙ‚Ø¹ Ù„Ù„Ù…Ø¨Ø§Ø±Ø§Ø© {match_id}")
    
    def calculate_accuracy(self):
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª"""
        evaluated_predictions = [p for p in self.predictions.values() if p['evaluated']]
        
        if not evaluated_predictions:
            return 0, 0, 0
        
        result_accuracy = 0
        goals_accuracy = 0
        total_matches = len(evaluated_predictions)
        
        for prediction in evaluated_predictions:
            pred = prediction['prediction']
            actual = prediction['actual_result']
            
            # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if (pred.get('score_prediction', {}).get('home_goals', 0) > 
                pred.get('score_prediction', {}).get('away_goals', 0) and
                actual.get('FTHG', 0) > actual.get('FTAG', 0)):
                result_accuracy += 1
            elif (pred.get('score_prediction', {}).get('home_goals', 0) < 
                  pred.get('score_prediction', {}).get('away_goals', 0) and
                  actual.get('FTHG', 0) < actual.get('FTAG', 0)):
                result_accuracy += 1
            elif (pred.get('score_prediction', {}).get('home_goals', 0) == 
                  pred.get('score_prediction', {}).get('away_goals', 0) and
                  actual.get('FTHG', 0) == actual.get('FTAG', 0)):
                result_accuracy += 1
            
            # Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
            goals_error = (abs(pred.get('score_prediction', {}).get('home_goals', 0) - actual.get('FTHG', 0)) +
                          abs(pred.get('score_prediction', {}).get('away_goals', 0) - actual.get('FTAG', 0)))
            goals_accuracy += (4 - min(goals_error, 3)) / 4  # ØªØ³Ø¬ÙŠÙ„ Ù…Ù† 0-1
        
        result_accuracy /= total_matches
        goals_accuracy /= total_matches
        overall_accuracy = (result_accuracy + goals_accuracy) / 2
        
        return result_accuracy, goals_accuracy, overall_accuracy
    
    def generate_performance_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª"""
        result_acc, goals_acc, overall_acc = self.calculate_accuracy()
        total_predictions = len(self.predictions)
        evaluated_predictions = len([p for p in self.predictions.values() if p['evaluated']])
        
        print("\n" + "="*80)
        print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª")
        print("="*80)
        
        print(f"\nğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:")
        print(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª: {total_predictions}")
        print(f"â€¢ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙŠÙ…Ø©: {evaluated_predictions}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result_acc:.1%}")
        print(f"â€¢ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {goals_acc:.1%}")
        print(f"â€¢ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©: {overall_acc:.1%}")
        
        if evaluated_predictions > 0:
            print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©:")
            recent_predictions = sorted(
                [p for p in self.predictions.values() if p['evaluated']],
                key=lambda x: x['timestamp'],
                reverse=True
            )[:5]
            
            for i, pred in enumerate(recent_predictions, 1):
                match_pred = pred['prediction']
                actual = pred['actual_result']
                
                home_team = match_pred.get('home_team', 'Unknown')
                away_team = match_pred.get('away_team', 'Unknown')
                pred_score = f"{match_pred.get('score_prediction', {}).get('home_goals', 0)}-{match_pred.get('score_prediction', {}).get('away_goals', 0)}"
                actual_score = f"{actual.get('FTHG', 0)}-{actual.get('FTAG', 0)}"
                
                result_correct = "âœ…" if pred_score == actual_score else "âŒ"
                print(f"  {i}. {home_team} vs {away_team}: {pred_score} (ÙØ¹Ù„ÙŠ: {actual_score}) {result_correct}")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„
if __name__ == "__main__":
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¹Ù‚Ø¨ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
    tracker = PredictionTracker()
    
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ ØªÙˆÙ‚Ø¹
    sample_prediction = {
        'home_team': 'Man City',
        'away_team': 'Liverpool', 
        'score_prediction': {'home_goals': 2, 'away_goals': 1},
        'scenario_probabilities': {'home_win': 0.6, 'draw': 0.2, 'away_win': 0.2}
    }
    
    sample_actual = {
        'FTHG': 2,
        'FTAG': 1,
        'FTR': 'H'
    }
    
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹ ÙˆØ§Ù„Ù†ØªÙŠØ¬Ø©
    match_id = f"Man City_vs_Liverpool_{datetime.now().strftime('%Y%m%d')}"
    tracker.record_prediction(match_id, sample_prediction, sample_actual)
    
    # Ø¹Ø±Ø¶ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
    tracker.generate_performance_report()