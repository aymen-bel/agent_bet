# smart_optimizer.py
import numpy as np
import pandas as pd
import json
from typing import Dict, List, Tuple, Optional, Callable
import random
from datetime import datetime
import logging
import os

class SmartWeightOptimizer:
    def __init__(self, state_dim: int = 20, learning_rate: float = 0.01):
        self.state_dim = state_dim
        self.learning_rate = learning_rate
        self.weights = self.initialize_weights()
        self.weights_history = []
        self.performance_history = []
        self.best_weights = self.weights.copy()
        self.best_reward = -np.inf
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        self.exploration_rate = 1.0
        self.min_exploration = 0.01
        self.exploration_decay = 0.995
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¹Ù„Ù…
        self.memory = []
        self.memory_size = 1000
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.output_dir = "output/reinforcement_learning"
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(f"{self.output_dir}/episodes", exist_ok=True)
    
    def initialize_weights(self) -> Dict[str, float]:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ø¹ Ù‚ÙŠÙˆØ¯ ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        return {
            # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            'points_per_match': np.random.uniform(0.1, 0.3),
            'win_rate': np.random.uniform(0.1, 0.25),
            'goal_difference': np.random.uniform(0.05, 0.15),
            
            # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù‡Ø¬ÙˆÙ…
            'shot_efficiency': np.random.uniform(0.05, 0.1),
            'conversion_rate': np.random.uniform(0.05, 0.1),
            'attacking_pressure': np.random.uniform(0.03, 0.08),
            
            # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¯ÙØ§Ø¹
            'defensive_efficiency': np.random.uniform(0.05, 0.15),
            'clean_sheet_rate': np.random.uniform(0.03, 0.08),
            'goals_conceded_per_match': np.random.uniform(0.05, 0.12),
            
            # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚ ÙˆØ§Ù„Ø´ÙƒÙ„
            'consistency_score': np.random.uniform(0.03, 0.08),
            'current_form': np.random.uniform(0.05, 0.1),
            'form_momentum': np.random.uniform(0.02, 0.06),
            
            # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
            'motivation_factor': np.random.uniform(0.02, 0.06),
            'external_factor': np.random.uniform(0.01, 0.04),
            'home_advantage': np.random.uniform(0.03, 0.08),
            
            # Ø£ÙˆØ²Ø§Ù† Ø¥Ø¶Ø§ÙÙŠØ©
            'opponent_strength': np.random.uniform(0.02, 0.06),
            'performance_trend': np.random.uniform(0.01, 0.04),
            'pressure_handling': np.random.uniform(0.01, 0.03),
            'attacking_consistency': np.random.uniform(0.02, 0.05),
            'defensive_consistency': np.random.uniform(0.02, 0.05)
        }
    
    def normalize_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡Ø§ 1"""
        total = sum(weights.values())
        if total == 0:
            return weights
        return {k: v / total for k, v in weights.items()}
    
    def get_state_features(self, team_metrics: Dict, opponent_metrics: Dict, context: Dict) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù…Ø§Øª Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²"""
        features = []
        
        # Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø³Ø¨ÙŠ
        features.append(team_metrics.get('points_per_match', 0))
        features.append(opponent_metrics.get('points_per_match', 0))
        features.append(team_metrics.get('win_rate', 0) - opponent_metrics.get('win_rate', 0))
        
        # Ø³Ù…Ø§Øª Ø§Ù„Ù‡Ø¬ÙˆÙ… ÙˆØ§Ù„Ø¯ÙØ§Ø¹
        features.append(team_metrics.get('goals_per_match', 0))
        features.append(opponent_metrics.get('goals_conceded_per_match', 0))
        features.append(team_metrics.get('defensive_efficiency', 0))
        features.append(opponent_metrics.get('shot_efficiency', 0))
        
        # Ø³Ù…Ø§Øª Ø§Ù„Ø´ÙƒÙ„ ÙˆØ§Ù„Ø§ØªØ³Ø§Ù‚
        features.append(team_metrics.get('current_form', 0))
        features.append(opponent_metrics.get('current_form', 0))
        features.append(team_metrics.get('consistency_score', 0))
        
        # Ø³Ù…Ø§Øª Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        features.append(team_metrics.get('motivation_factor', 1.0))
        features.append(opponent_metrics.get('motivation_factor', 1.0))
        features.append(context.get('home_advantage', 1.0))
        features.append(context.get('importance_multiplier', 1.0))
        
        # Ø³Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        features.append(team_metrics.get('performance_trend', 0))
        features.append(team_metrics.get('attacking_consistency', 0))
        features.append(team_metrics.get('defensive_consistency', 0))
        
        # Ù…Ù„Ø¡ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        while len(features) < self.state_dim:
            features.append(0.0)
        
        return np.array(features[:self.state_dim])
    
    def select_action(self, state: np.ndarray) -> Dict[str, float]:
        """Ø§Ø®ØªÙŠØ§Ø± action (ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Îµ-greedy"""
        if random.random() < self.exploration_rate:
            # Ø§Ø³ØªÙƒØ´Ø§Ù: ØªØ¹Ø¯ÙŠÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„Ø£ÙˆØ²Ø§Ù†
            return self.explore_weights()
        else:
            # Ø§Ø³ØªØºÙ„Ø§Ù„: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø³Ø§Ø¨Ù‚
            return self.exploit_weights(state)
    
    def explore_weights(self) -> Dict[str, float]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù: ØªØ¹Ø¯ÙŠÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„Ø£ÙˆØ²Ø§Ù†"""
        new_weights = self.weights.copy()
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        weights_to_modify = random.sample(list(new_weights.keys()), 
                                        k=random.randint(3, 8))
        
        for weight_key in weights_to_modify:
            modification = np.random.normal(0, 0.1)  # ØªØ¹Ø¯ÙŠÙ„ ØµØºÙŠØ±
            new_weights[weight_key] = max(0.01, new_weights[weight_key] + modification)
        
        return self.normalize_weights(new_weights)
    
    def exploit_weights(self, state: np.ndarray) -> Dict[str, float]:
        """Ø§Ø³ØªØºÙ„Ø§Ù„: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…"""
        new_weights = self.weights.copy()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØªØ¹Ø¯ÙŠÙ„
        state_analysis = self.analyze_state(state)
        
        for weight_key, adjustment in state_analysis.items():
            new_weights[weight_key] = max(0.01, new_weights[weight_key] + adjustment)
        
        return self.normalize_weights(new_weights)
    
    def analyze_state(self, state: np.ndarray) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„ØªØ­Ø¯ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"""
        adjustments = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ø§Ù„Ø¯ÙˆÙ„Ø©
        if state[0] > 0.6:  # Ù†Ù‚Ø§Ø· Ø¹Ø§Ù„ÙŠØ©
            adjustments['points_per_match'] = 0.05
        if state[2] > 0.2:  # ÙØ±Ù‚ ÙÙˆØ² ÙƒØ¨ÙŠØ±
            adjustments['win_rate'] = 0.03
        if state[4] < 1.0:  # Ø¯ÙØ§Ø¹ Ø¶Ø¹ÙŠÙ Ù„Ù„Ø®ØµÙ…
            adjustments['shot_efficiency'] = 0.02
        if state[10] > 1.1:  # ØªØ­ÙÙŠØ² Ø¹Ø§Ù„ÙŠ
            adjustments['motivation_factor'] = 0.01
        
        return adjustments
    
    def calculate_reward(self, predictions: List[Dict], actual_results: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        if not predictions or not actual_results:
            return 0.0
        
        reward = 0.0
        total_weight = 0
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        result_accuracy = self.calculate_result_accuracy(predictions, actual_results)
        reward += result_accuracy * 0.4
        total_weight += 0.4
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
        score_accuracy = self.calculate_score_accuracy(predictions, actual_results)
        reward += score_accuracy * 0.3
        total_weight += 0.3
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„ÙØ§Ø±Ù‚
        diff_accuracy = self.calculate_difference_accuracy(predictions, actual_results)
        reward += diff_accuracy * 0.2
        total_weight += 0.2
        
        # Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„Ø«Ù‚Ø© (Ø¹Ù‚Ø§Ø¨ Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©)
        confidence_penalty = self.calculate_confidence_penalty(predictions, actual_results)
        reward -= confidence_penalty * 0.1
        total_weight += 0.1
        
        if total_weight > 0:
            reward /= total_weight
        
        return max(0.0, reward)
    
    def calculate_result_accuracy(self, predictions: List[Dict], actual: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© (ÙÙˆØ²/ØªØ¹Ø§Ø¯Ù„/Ø®Ø³Ø§Ø±Ø©)"""
        if not predictions:
            return 0.0
        
        best_pred = predictions[0]
        pred_home = best_pred.get('home_goals', 0)
        pred_away = best_pred.get('away_goals', 0)
        actual_home = actual.get('home_goals', 0)
        actual_away = actual.get('away_goals', 0)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØ§Ù„ÙØ¹Ù„ÙŠØ©
        pred_result = 'H' if pred_home > pred_away else 'A' if pred_away > pred_home else 'D'
        actual_result = 'H' if actual_home > actual_away else 'A' if actual_away > actual_home else 'D'
        
        return 1.0 if pred_result == actual_result else 0.0
    
    def calculate_score_accuracy(self, predictions: List[Dict], actual: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©"""
        if not predictions:
            return 0.0
        
        best_pred = predictions[0]
        pred_home = best_pred.get('home_goals', 0)
        pred_away = best_pred.get('away_goals', 0)
        actual_home = actual.get('home_goals', 0)
        actual_away = actual.get('away_goals', 0)
        
        if pred_home == actual_home and pred_away == actual_away:
            return 1.0
        elif abs(pred_home - actual_home) <= 1 and abs(pred_away - actual_away) <= 1:
            return 0.5
        else:
            return 0.0
    
    def calculate_difference_accuracy(self, predictions: List[Dict], actual: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ØªÙˆÙ‚Ø¹ ÙØ§Ø±Ù‚ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù"""
        if not predictions:
            return 0.0
        
        best_pred = predictions[0]
        pred_diff = best_pred.get('home_goals', 0) - best_pred.get('away_goals', 0)
        actual_diff = actual.get('home_goals', 0) - actual.get('away_goals', 0)
        
        if (pred_diff > 0 and actual_diff > 0) or (pred_diff < 0 and actual_diff < 0) or (pred_diff == 0 and actual_diff == 0):
            return 1.0
        else:
            return 0.0
    
    def calculate_confidence_penalty(self, predictions: List[Dict], actual: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© (Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ø®Ø§Ø·Ø¦Ø©)"""
        if not predictions:
            return 0.0
        
        best_pred = predictions[0]
        confidence = best_pred.get('confidence', 0.5)
        
        # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ Ø®Ø§Ø·Ø¦Ø§Ù‹
        pred_home = best_pred.get('home_goals', 0)
        pred_away = best_pred.get('away_goals', 0)
        actual_home = actual.get('home_goals', 0)
        actual_away = actual.get('away_goals', 0)
        
        is_correct = (pred_home == actual_home and pred_away == actual_away)
        
        if not is_correct and confidence > 0.7:
            return confidence  # Ø¹Ù‚Ø§Ø¨ Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªÙ†Ø¨Ø¤ Ø®Ø§Ø·Ø¦
        
        return 0.0
    
    def update_weights(self, reward: float, next_state: np.ndarray):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©"""
        # ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        self.exploration_rate = max(self.min_exploration, 
                                  self.exploration_rate * self.exploration_decay)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
        learning_adjustment = self.learning_rate * reward
        
        for key in self.weights:
            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙŠ Ø³Ø§Ù‡Ù…Øª ÙÙŠ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
            if reward > 0:
                self.weights[key] += learning_adjustment * random.uniform(0.8, 1.2)
            else:
                self.weights[key] -= learning_adjustment * random.uniform(0.8, 1.2)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ù‚ÙˆÙ„Ø©
            self.weights[key] = max(0.01, min(0.3, self.weights[key]))
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self.weights = self.normalize_weights(self.weights)
        
        # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        if reward > self.best_reward:
            self.best_reward = reward
            self.best_weights = self.weights.copy()
            self.logger.info(f"ğŸš€ Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø© Ø¬Ø¯ÙŠØ¯Ø©: {reward:.3f}")
    
    def train(self, training_data: List[Dict], episodes: int = 1000, callback: Optional[Callable] = None):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²"""
        self.logger.info(f"ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {episodes} Ø­Ù„Ù‚Ø©...")
        
        for episode in range(episodes):
            episode_reward = 0
            matches_trained = 0
            episode_actions = []
            episode_states = []
            
            for match_data in training_data:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                team_metrics = match_data['team_metrics']
                opponent_metrics = match_data['opponent_metrics']
                context = match_data['context']
                actual_result = match_data['actual_result']
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ù…Ø§Øª Ø§Ù„Ø¯ÙˆÙ„Ø©
                state = self.get_state_features(team_metrics, opponent_metrics, context)
                episode_states.append(state.tolist())
                
                # Ø§Ø®ØªÙŠØ§Ø± action (ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
                action_weights = self.select_action(state)
                episode_actions.append({k: v for k, v in action_weights.items()})
                
                # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                predictions = self.generate_predictions(team_metrics, opponent_metrics, context, action_weights)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
                reward = self.calculate_reward(predictions, actual_result)
                episode_reward += reward
                matches_trained += 1
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                next_state = self.get_state_features(team_metrics, opponent_metrics, context)
                self.update_weights(reward, next_state)
                
                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.remember(state, action_weights, reward, next_state)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ù„Ù‚Ø©
            avg_reward = episode_reward / matches_trained if matches_trained > 0 else 0
            self.performance_history.append(avg_reward)
            self.weights_history.append(self.weights.copy())
            
            # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ callback Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if callback:
                callback(episode, avg_reward, self.exploration_rate, episode_actions[:5], episode_states[:3])
            
            # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù„Ù‚Ø©
            self._save_episode_data(episode, avg_reward, self.exploration_rate)
            
            if episode % 100 == 0:
                self.logger.info(f"ğŸ“Š Ø§Ù„Ø­Ù„Ù‚Ø© {episode}: Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© = {avg_reward:.3f}, Ø§Ø³ØªÙƒØ´Ø§Ù = {self.exploration_rate:.3f}")
        
        self.logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø©: {self.best_reward:.3f}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        self.save_training_results()
    
    def _save_episode_data(self, episode: int, reward: float, exploration_rate: float):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù„Ù‚Ø©"""
        episode_data = {
            'episode_number': episode,
            'timestamp': datetime.now().isoformat(),
            'reward_achieved': reward,
            'exploration_rate': exploration_rate,
            'current_weights': {k: round(v, 4) for k, v in self.weights.items()}
        }
        
        filename = f"{self.output_dir}/episodes/episode_{episode:03d}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(episode_data, f, indent=2, ensure_ascii=False)
    
    def save_training_results(self):
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        results = {
            'best_weights': self.best_weights,
            'best_reward': self.best_reward,
            'final_weights': self.weights,
            'performance_history': self.performance_history,
            'training_parameters': {
                'state_dim': self.state_dim,
                'learning_rate': self.learning_rate,
                'min_exploration': self.min_exploration,
                'exploration_decay': self.exploration_decay
            },
            'timestamp': datetime.now().isoformat()
        }
        
        filename = f"{self.output_dir}/rl_training_results.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ: {filename}")
    
    def remember(self, state, action, reward, next_state):
        """Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        self.memory.append((state, action, reward, next_state))
        if len(self.memory) > self.memory_size:
            self.memory.pop(0)
    
    def generate_predictions(self, team_metrics: Dict, opponent_metrics: Dict, context: Dict, weights: Dict) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©"""
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        team_strength = self.calculate_weighted_strength(team_metrics, weights)
        opponent_strength = self.calculate_weighted_strength(opponent_metrics, weights)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚
        home_advantage = context.get('home_advantage', 1.1)
        motivation_boost = (team_metrics.get('motivation_factor', 1.0) + 
                          opponent_metrics.get('motivation_factor', 1.0)) / 2
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
        base_goals = 1.4  # Ù…ØªÙˆØ³Ø· Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø¯ÙˆØ±ÙŠ
        home_expected = team_strength * (1 / opponent_strength) * base_goals * home_advantage
        away_expected = opponent_strength * (1 / team_strength) * base_goals / home_advantage
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ­ÙÙŠØ²
        home_expected *= team_metrics.get('motivation_factor', 1.0)
        away_expected *= opponent_metrics.get('motivation_factor', 1.0)
        
        # Ø¶Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        home_expected = max(0.2, min(3.5, home_expected))
        away_expected = max(0.2, min(3.0, away_expected))
        
        # ØªÙˆÙ„ÙŠØ¯ ØªÙ†Ø¨Ø¤Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
        predictions = []
        for _ in range(3):
            home_goals = np.random.poisson(home_expected)
            away_goals = np.random.poisson(away_expected)
            
            predictions.append({
                'home_goals': int(home_goals),
                'away_goals': int(away_goals),
                'confidence': np.random.uniform(0.3, 0.8),
                'type': self.classify_prediction(home_goals, away_goals)
            })
        
        return predictions
    
    def calculate_weighted_strength(self, metrics: Dict, weights: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù…Ø±Ø¬Ø­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        strength = 0.0
        
        for metric_key, weight in weights.items():
            if metric_key in metrics:
                metric_value = metrics[metric_key]
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ù…Ù† 0-1 Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                if isinstance(metric_value, (int, float)) and metric_value > 1:
                    metric_value = metric_value / 100
                strength += metric_value * weight
        
        return max(0.1, strength)
    
    def classify_prediction(self, home_goals: int, away_goals: int) -> str:
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        goal_diff = abs(home_goals - away_goals)
        total_goals = home_goals + away_goals
        
        if goal_diff <= 1:
            return "ØªÙ†Ø¨Ø¤ Ø¢Ù…Ù†"
        elif total_goals >= 5:
            return "Ù†ØªÙŠØ¬Ø© Ø¹Ø§Ù„ÙŠØ©"
        elif goal_diff >= 3:
            return "Ù†ØªÙŠØ¬Ø© ÙƒØ¨ÙŠØ±Ø©"
        else:
            return "ØªÙ†Ø¨Ø¤ Ù…Ø¹Ù‚ÙˆÙ„"
    
    def get_optimal_weights(self) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø«Ù„Ù‰"""
        return self.best_weights
    
    def save_weights(self, filepath: str):
        """Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙÙŠ Ù…Ù„Ù"""
        weights_data = {
            'weights': self.best_weights,
            'performance_history': self.performance_history,
            'best_reward': self.best_reward,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(weights_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙÙŠ: {filepath}")
    
    def load_weights(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù…Ù† Ù…Ù„Ù"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                weights_data = json.load(f)
            
            self.best_weights = weights_data['weights']
            self.weights = self.best_weights.copy()
            self.performance_history = weights_data.get('performance_history', [])
            self.best_reward = weights_data.get('best_reward', 0)
            
            self.logger.info(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù…Ù†: {filepath}")
        except FileNotFoundError:
            self.logger.warning(f"âš ï¸  Ù…Ù„Ù Ø§Ù„Ø£ÙˆØ²Ø§Ù† ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {filepath}")